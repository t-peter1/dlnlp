python train.py
2026-01-13 17:52:32.942192: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-13 17:52:33.015188: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
Freezing base model weights...
Replaced transformer.h.0.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.1.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.2.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.3.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.4.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.5.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.6.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.7.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.8.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.9.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.10.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.11.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.12.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.13.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.14.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.15.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.16.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.17.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.18.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.19.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.20.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.21.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.22.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.23.attn.c_attn with LoRA layer (r=4)
--- Parameter Check ---
trainable params: 393216 || all params: 355216384 || trainable%: 0.1107
Starting training...
  0%|                                                                                                                  | 0/1875 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
{'loss': 2.0227, 'grad_norm': 5.159192085266113, 'learning_rate': 0.00019477333333333335, 'epoch': 0.08}                                        
{'loss': 1.2844, 'grad_norm': 5.105502605438232, 'learning_rate': 0.00018944000000000003, 'epoch': 0.16}                                        
{'loss': 1.0852, 'grad_norm': 5.910290718078613, 'learning_rate': 0.00018410666666666668, 'epoch': 0.24}                                        
{'loss': 1.0387, 'grad_norm': 5.443533897399902, 'learning_rate': 0.00017877333333333334, 'epoch': 0.32}                                        
{'loss': 0.9849, 'grad_norm': 4.966643333435059, 'learning_rate': 0.00017344, 'epoch': 0.4}                                                     
{'loss': 0.9556, 'grad_norm': 4.382678031921387, 'learning_rate': 0.00016810666666666667, 'epoch': 0.48}                                        
{'loss': 0.952, 'grad_norm': 4.348861217498779, 'learning_rate': 0.00016277333333333333, 'epoch': 0.56}                                         
{'loss': 0.9586, 'grad_norm': 4.1466803550720215, 'learning_rate': 0.00015744, 'epoch': 0.64}                                                   
{'loss': 0.9266, 'grad_norm': 4.174616813659668, 'learning_rate': 0.00015210666666666666, 'epoch': 0.72}                                        
{'loss': 0.8962, 'grad_norm': 4.103938102722168, 'learning_rate': 0.00014677333333333335, 'epoch': 0.8}                                         
{'loss': 0.8925, 'grad_norm': 3.9128615856170654, 'learning_rate': 0.00014144000000000003, 'epoch': 0.88}                                       
{'loss': 0.8731, 'grad_norm': 4.064955711364746, 'learning_rate': 0.00013610666666666668, 'epoch': 0.96}                                        
{'eval_loss': 0.9073099493980408, 'eval_runtime': 7.4559, 'eval_samples_per_second': 67.061, 'eval_steps_per_second': 8.45, 'epoch': 1.0}       
{'loss': 0.8667, 'grad_norm': 4.0362749099731445, 'learning_rate': 0.00013077333333333334, 'epoch': 1.04}                                       
{'loss': 0.8618, 'grad_norm': 4.236410140991211, 'learning_rate': 0.00012544, 'epoch': 1.12}                                                    
{'loss': 0.8691, 'grad_norm': 3.9549686908721924, 'learning_rate': 0.00012010666666666667, 'epoch': 1.2}                                        
{'loss': 0.8646, 'grad_norm': 4.094362735748291, 'learning_rate': 0.00011477333333333333, 'epoch': 1.28}                                        
{'loss': 0.8497, 'grad_norm': 3.774531126022339, 'learning_rate': 0.00010944000000000001, 'epoch': 1.36}                                        
{'loss': 0.8319, 'grad_norm': 3.734027147293091, 'learning_rate': 0.00010410666666666666, 'epoch': 1.44}                                        
{'loss': 0.8545, 'grad_norm': 7.871815204620361, 'learning_rate': 9.877333333333335e-05, 'epoch': 1.52}                                         
{'loss': 0.8257, 'grad_norm': 3.2337005138397217, 'learning_rate': 9.344e-05, 'epoch': 1.6}                                                     
{'loss': 0.8401, 'grad_norm': 3.8105194568634033, 'learning_rate': 8.810666666666667e-05, 'epoch': 1.68}                                        
{'loss': 0.8182, 'grad_norm': 3.4072890281677246, 'learning_rate': 8.277333333333334e-05, 'epoch': 1.76}                                        
{'loss': 0.828, 'grad_norm': 4.001256465911865, 'learning_rate': 7.744e-05, 'epoch': 1.84}                                                      
{'loss': 0.8034, 'grad_norm': 3.51187801361084, 'learning_rate': 7.210666666666667e-05, 'epoch': 1.92}                                          
{'loss': 0.8235, 'grad_norm': 3.7538821697235107, 'learning_rate': 6.677333333333333e-05, 'epoch': 2.0}                                         
{'eval_loss': 0.8654030561447144, 'eval_runtime': 7.4672, 'eval_samples_per_second': 66.959, 'eval_steps_per_second': 8.437, 'epoch': 2.0}      
{'loss': 0.824, 'grad_norm': 3.9206459522247314, 'learning_rate': 6.144e-05, 'epoch': 2.08}                                                     
{'loss': 0.814, 'grad_norm': 4.0967793464660645, 'learning_rate': 5.6106666666666676e-05, 'epoch': 2.16}                                        
{'loss': 0.789, 'grad_norm': 3.302777051925659, 'learning_rate': 5.077333333333334e-05, 'epoch': 2.24}                                          
{'loss': 0.8167, 'grad_norm': 3.596801519393921, 'learning_rate': 4.5440000000000005e-05, 'epoch': 2.32}                                        
{'loss': 0.7869, 'grad_norm': 3.6947362422943115, 'learning_rate': 4.0106666666666673e-05, 'epoch': 2.4}                                        
{'loss': 0.7819, 'grad_norm': 2.991231679916382, 'learning_rate': 3.4773333333333335e-05, 'epoch': 2.48}                                        
{'loss': 0.7811, 'grad_norm': 3.3027453422546387, 'learning_rate': 2.944e-05, 'epoch': 2.56}                                                    
{'loss': 0.7846, 'grad_norm': 3.3316099643707275, 'learning_rate': 2.4106666666666667e-05, 'epoch': 2.64}                                       
{'loss': 0.7895, 'grad_norm': 3.9573686122894287, 'learning_rate': 1.8773333333333335e-05, 'epoch': 2.72}                                       
{'loss': 0.8032, 'grad_norm': 3.72784686088562, 'learning_rate': 1.344e-05, 'epoch': 2.8}                                                       
{'loss': 0.7778, 'grad_norm': 3.9326109886169434, 'learning_rate': 8.106666666666666e-06, 'epoch': 2.88}                                        
{'loss': 0.7764, 'grad_norm': 3.626605272293091, 'learning_rate': 2.773333333333333e-06, 'epoch': 2.96}                                         
{'eval_loss': 0.8569201827049255, 'eval_runtime': 7.4997, 'eval_samples_per_second': 66.669, 'eval_steps_per_second': 8.4, 'epoch': 3.0}        
{'train_runtime': 567.9034, 'train_samples_per_second': 26.413, 'train_steps_per_second': 3.302, 'train_loss': 0.8996913421630859, 'epoch': 3.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [09:27<00:00,  3.30it/s]
(base) jovyan@jupyter-e12426897:~/dlnlp/dlnlp$ import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from src.model import inject_lora

# 1. Setup
MODEL_ID = "gpt2-medium"
device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = GPT2Tokenizer.from_pretrained(MODEL_ID)

# 2. Load the Base Model
print("Loading base model...")
model = GPT2LMHeadModel.from_pretrained(MODEL_ID)

# 3. Inject the LoRA Layers (Structure must match training!)
model = inject_lora(model, rank=4, alpha=32)

# 4. Load your Trained Weights
print("Loading trained LoRA weights...")
# Note: strict=False allows us to load even if some keys are missing/extra
# (useful since we are mixing frozen and trained weights)
model.load_state_dict(torch.load("lora_weights_only.pt"), strict=False)
model.to(device)
model.eval()

# 5. Test It
# Input: A structured meaning representation (MR)
text_input = "name[The Eagle] || eatType[coffee shop] || food[Japanese] || priceRange[cheap]"
print(f"\nINPUT: {text_input}")

input_ids = tokenizer.encode(text_input, return_tensors='pt').to(device)

# Generate
print("Generating...")
with torch.no_grad():
    output_tokens = model.generate(
        input_ids, 
        max_length=100, 
        num_return_sequences=1,
                                         ^C
(base) jovyan@jupyter-e12426897:~/dlnlp/dlnlp$ python generate.py
2026-01-13 18:02:44.224917: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-13 18:02:44.298698: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.
  warnings.warn(
Loading base model...
Replaced transformer.h.0.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.1.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.2.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.3.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.4.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.5.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.6.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.7.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.8.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.9.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.10.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.11.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.12.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.13.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.14.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.15.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.16.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.17.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.18.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.19.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.20.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.21.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.22.attn.c_attn with LoRA layer (r=4)
Replaced transformer.h.23.attn.c_attn with LoRA layer (r=4)
Loading trained LoRA weights...

INPUT: name[The Eagle] || eatType[coffee shop] || food[Japanese] || priceRange[cheap]
Generating...
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
------------------------------
OUTPUT: name[The Eagle] || eatType[coffee shop] || food[Japanese] || priceRange[cheap] || The Eagle coffee shop offers cheap Japanese food    which is offered by family of regulars  in the  area of North Cambridge.   It is in the price range of £20.  Price range is slightly higher than average.  Family members also have  Japanese restaurant.  Customer rating is low however.  It has a 1 out of 5. 
------------------------------